{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WWT Unravel 2025 â€” Final Hybrid Recommender Notebook\n",
        "\n",
        "This notebook loads your CSVs, builds a hybrid recommender (co-occurrence + FP-Growth rules + Item2Vec embeddings + popularity), performs a light grid search to tune weights, and writes the submission Excel.\n",
        "\n",
        "**IMPORTANT:** Upload `order_data.csv`, `customer_data.csv`, `store_data.csv`, and `test_data_question.csv` into `/content/sample_data` in Colab before running. If `order_data.csv` is large, runtime may take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "!pip install --quiet pandas numpy scikit-learn scipy openpyxl xlsxwriter tqdm mlxtend gensim annoy\n",
        "print('Packages installed or already present.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and robust CSV reader\n",
        "import os, json, re, random, shutil, math\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "print('Imports ok.')\n",
        "\n",
        "def robust_read_csv(path):\n",
        "    try:\n",
        "        df = pd.read_csv(path, low_memory=False, engine='python', encoding='utf-8', on_bad_lines='skip')\n",
        "        print(f'Loaded {path} ->', df.shape)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print('utf-8 failed:', e)\n",
        "    try:\n",
        "        df = pd.read_csv(path, low_memory=False, engine='python', encoding='latin1', on_bad_lines='skip')\n",
        "        print(f'Loaded {path} with latin1 ->', df.shape)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print('latin1 failed:', e)\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File paths (adjust if you stored files elsewhere)\n",
        "order_path = '/content/sample_data/order_data.csv'\n",
        "customer_path = '/content/sample_data/customer_data.csv'\n",
        "store_path = '/content/sample_data/store_data.csv'\n",
        "test_path = '/content/sample_data/test_data_question.csv'\n",
        "\n",
        "print('Files exist and sizes:')\n",
        "for p in [order_path, customer_path, store_path, test_path]:\n",
        "    if os.path.exists(p):\n",
        "        print(p, '->', f\"{os.path.getsize(p)/(1024*1024):.2f} MB\")\n",
        "    else:\n",
        "        print(p, '-> MISSING')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "test_df = robust_read_csv(test_path)\n",
        "\n",
        "order_df = None\n",
        "customer_df = None\n",
        "store_df = None\n",
        "\n",
        "if os.path.exists(order_path):\n",
        "    try:\n",
        "        order_df = robust_read_csv(order_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to load order_data.csv:', e)\n",
        "else:\n",
        "    print('order_data.csv not found; proceeding with test file only (lower accuracy expected).')\n",
        "\n",
        "if os.path.exists(customer_path):\n",
        "    try:\n",
        "        customer_df = robust_read_csv(customer_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to load customer_data.csv:', e)\n",
        "\n",
        "if os.path.exists(store_path):\n",
        "    try:\n",
        "        store_df = robust_read_csv(store_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to load store_data.csv:', e)\n",
        "\n",
        "print('\\nSummary shapes:')\n",
        "print('order_df', None if order_df is None else order_df.shape)\n",
        "print('customer_df', None if customer_df is None else customer_df.shape)\n",
        "print('store_df', None if store_df is None else store_df.shape)\n",
        "print('test_df', test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse and normalize transactions from order_df and test_df\n",
        "def split_items(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    s = str(s).strip()\n",
        "    if s.startswith('[') and s.endswith(']'):\n",
        "        try:\n",
        "            arr = json.loads(s.replace(\"'\", '\"'))\n",
        "            return [str(x).strip() for x in arr if str(x).strip()]\n",
        "        except:\n",
        "            pass\n",
        "    for delim in ['|', '||', ';', ',', '\\n', '/', ' + ', ' / ']:\n",
        "        if delim in s:\n",
        "            parts = [p.strip() for p in s.split(delim) if p.strip()]\n",
        "            if len(parts) > 0:\n",
        "                return parts\n",
        "    return [s] if s else []\n",
        "\n",
        "import re\n",
        "def normalize_item(it):\n",
        "    s = str(it).strip().lower()\n",
        "    # remove common size tokens (customize to your needs)\n",
        "    s = re.sub(r'\\b(large|medium|small|xl|lrg|sm|20 oz|16 oz|12 oz|oz|ml|g|pack|pc|pcs)\\b', '', s, flags=re.I)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "transactions = []\n",
        "\n",
        "# From order_df if present\n",
        "if order_df is not None and 'ORDERS' in order_df.columns:\n",
        "    for s in order_df['ORDERS'].astype(str).values:\n",
        "        parts = split_items(s)\n",
        "        if parts:\n",
        "            tx = [normalize_item(x) for x in parts if str(x).strip()]\n",
        "            if tx:\n",
        "                transactions.append(tx)\n",
        "print('Transactions from order_df (if loaded):', len(transactions))\n",
        "\n",
        "# From test_df item columns\n",
        "item_cols = [c for c in test_df.columns if c.lower().startswith('item')]\n",
        "print('Detected item columns in test file:', item_cols)\n",
        "for _, row in test_df.iterrows():\n",
        "    tx = []\n",
        "    for c in item_cols:\n",
        "        v = row.get(c)\n",
        "        if pd.isna(v): continue\n",
        "        s = str(v).strip()\n",
        "        if s == '' or s.lower() == 'missing': continue\n",
        "        tx.append(normalize_item(s))\n",
        "    if tx:\n",
        "        transactions.append(tx)\n",
        "\n",
        "print('Total transactions prepared (order + test):', len(transactions))\n",
        "print('Example transaction:', transactions[0] if transactions else 'NONE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build vocab and sparse transaction matrix in sparse format (memory efficient)\n",
        "from collections import Counter\n",
        "from scipy import sparse as sp\n",
        "min_support = 1  # keep rare items to avoid losing test items; set to 2+ only if memory is critical\n",
        "item_counts = Counter([it for tx in transactions for it in tx])\n",
        "vocab = sorted([it for it, c in item_counts.items() if c >= min_support])\n",
        "print('Vocab size:', len(vocab))\n",
        "\n",
        "item_to_idx = {it: i for i, it in enumerate(vocab)}\n",
        "idx_to_item = {i: it for it, i in item_to_idx.items()}\n",
        "\n",
        "rows, cols = [], []\n",
        "for r, tx in enumerate(transactions):\n",
        "    for it in tx:\n",
        "        if it in item_to_idx:\n",
        "            rows.append(r)\n",
        "            cols.append(item_to_idx[it])\n",
        "data = np.ones(len(rows), dtype=np.uint8)\n",
        "X_sparse = sp.csr_matrix((data, (rows, cols)), shape=(len(transactions), len(vocab)), dtype=np.uint8)\n",
        "print('Transaction matrix shape:', X_sparse.shape, 'NNZ:', X_sparse.nnz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build top-k co-occurrence similarity lists (cosine-like)\n",
        "import numpy as np\n",
        "item_counts_arr = np.array(X_sparse.sum(axis=0)).ravel().astype(float)\n",
        "n_items = X_sparse.shape[1]\n",
        "sim_topk = 50\n",
        "topk_sim_lists = {}\n",
        "eps = 1e-9\n",
        "\n",
        "for i in range(n_items):\n",
        "    v = X_sparse[:, i]\n",
        "    cooc = (X_sparse.T).dot(v).toarray().ravel()\n",
        "    cooc[i] = 0.0\n",
        "    denom = np.sqrt(item_counts_arr[i] * item_counts_arr + eps)\n",
        "    sim = cooc / (denom + eps)\n",
        "    top_idx = np.argsort(-sim)[:sim_topk]\n",
        "    topk_sim_lists[i] = [(int(j), float(sim[j])) for j in top_idx if sim[j] > 0]\n",
        "\n",
        "print('Built top-k similarity lists for items:', n_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global popularity vector\n",
        "item_counts_arr = np.array(X_sparse.sum(axis=0)).ravel().astype(float)\n",
        "global_pop = item_counts_arr.copy()\n",
        "if global_pop.max() > 0:\n",
        "    global_pop = global_pop / global_pop.max()\n",
        "\n",
        "# Store-level popularity (optional) - fallback to global_pop if store not present\n",
        "store_pop = {}\n",
        "if order_df is not None and 'STORE_NUMBER' in order_df.columns:\n",
        "    groups = order_df.groupby('STORE_NUMBER')\n",
        "    for store, g in groups:\n",
        "        all_items = []\n",
        "        col_candidates = [c for c in g.columns if 'order' in c.lower() or 'orders' in c.lower()]\n",
        "        if col_candidates:\n",
        "            col = col_candidates[0]\n",
        "            for s in g[col].astype(str).values:\n",
        "                parts = split_items(s)\n",
        "                all_items.extend([normalize_item(x) for x in parts if str(x).strip()])\n",
        "        cnt = Counter([it for it in all_items if it in item_to_idx])\n",
        "        vec = np.array([cnt.get(idx_to_item[i], 0) for i in range(n_items)], dtype=float)\n",
        "        if vec.max() > 0:\n",
        "            store_pop[store] = vec / vec.max()\n",
        "print('Store popularity vectors built for', len(store_pop), 'stores.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mine association rules using FP-Growth (mlxtend)\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_te = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "min_support_frac = 0.001  # tune as needed; lower -> more rules (and more compute)\n",
        "freq = fpgrowth(df_te, min_support=min_support_frac, use_colnames=True)\n",
        "rules = association_rules(freq, metric='confidence', min_threshold=0.25)\n",
        "rules['antecedent'] = rules['antecedents'].apply(lambda x: frozenset([a.lower() for a in x]))\n",
        "rules['consequent'] = rules['consequents'].apply(lambda x: [c.lower() for c in x])\n",
        "rules_dict = {}\n",
        "for _, r in rules.iterrows():\n",
        "    rules_dict.setdefault(r['antecedent'], []).append((r['consequent'], float(r['confidence'])))\n",
        "print('Extracted', len(freq), 'frequent itemsets and', len(rules), 'rules.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Word2Vec (Item2Vec)\n",
        "from gensim.models import Word2Vec\n",
        "vec_size = 64\n",
        "window = 5\n",
        "min_count = 1\n",
        "epochs = 12\n",
        "workers = 2\n",
        "\n",
        "sentences = transactions\n",
        "w2v = Word2Vec(sentences=sentences, vector_size=vec_size, window=window, min_count=min_count, sg=1, negative=5, epochs=epochs, workers=workers)\n",
        "print('Trained Word2Vec. Vocab size in model:', len(w2v.wv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build embedding matrix and normalize\n",
        "from sklearn.preprocessing import normalize\n",
        "n_items = len(vocab)\n",
        "embed_dim = vec_size\n",
        "item_emb_matrix = np.zeros((n_items, embed_dim), dtype=float)\n",
        "for it, idx in item_to_idx.items():\n",
        "    if it in w2v.wv:\n",
        "        item_emb_matrix[idx] = w2v.wv[it]\n",
        "    else:\n",
        "        item_emb_matrix[idx] = np.random.normal(scale=0.01, size=(embed_dim,))\n",
        "emb_norm = normalize(item_emb_matrix, axis=1)\n",
        "print('Embedding matrix built and normalized.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding-based scores for a cart\n",
        "def embedding_scores_for_cart(cart_items):\n",
        "    cart_idxs = [item_to_idx[it] for it in cart_items if it in item_to_idx]\n",
        "    if not cart_idxs:\n",
        "        return np.zeros(n_items, dtype=float)\n",
        "    cart_vecs = emb_norm[cart_idxs]\n",
        "    sim_mat = emb_norm.dot(cart_vecs.T)\n",
        "    avg_sim = sim_mat.mean(axis=1)\n",
        "    if avg_sim.max() > 0:\n",
        "        avg_sim = (avg_sim - avg_sim.min()) / (avg_sim.max() - avg_sim.min() + 1e-9)\n",
        "    return avg_sim\n",
        "print('Embedding score function ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hybrid recommender that combines similarity, rules, embedding, and popularity\n",
        "def recommend_with_embeddings(cart_items, top_k=3, alpha=0.5, beta=0.2, gamma=0.2, store_number=None):\n",
        "    cart_items = [it for it in cart_items if it in item_to_idx]\n",
        "    cart_idxs = [item_to_idx[it] for it in cart_items]\n",
        "    if len(cart_idxs) == 0:\n",
        "        top_pop = np.argsort(-global_pop)[:top_k]\n",
        "        return [idx_to_item[i] for i in top_pop]\n",
        "    # similarity score\n",
        "    score_sim = np.zeros(n_items, dtype=float)\n",
        "    for ci in cart_idxs:\n",
        "        for other_idx, s in topk_sim_lists.get(ci, []):\n",
        "            score_sim[other_idx] += s\n",
        "    if score_sim.max() > 0:\n",
        "        score_sim = score_sim / (score_sim.max() + 1e-9)\n",
        "    # rule score\n",
        "    score_rule = np.zeros(n_items, dtype=float)\n",
        "    cart_set = frozenset(cart_items)\n",
        "    for ante, recs in rules_dict.items():\n",
        "        if ante.issubset(cart_set):\n",
        "            for cons, conf in recs:\n",
        "                for c in cons:\n",
        "                    if c in item_to_idx:\n",
        "                        score_rule[item_to_idx[c]] = max(score_rule[item_to_idx[c]], conf)\n",
        "    if score_rule.max() > 0:\n",
        "        score_rule = score_rule / (score_rule.max() + 1e-9)\n",
        "    # embedding score\n",
        "    score_embed = embedding_scores_for_cart(cart_items)\n",
        "    # popularity (store fallback)\n",
        "    if store_number is not None and store_number in store_pop:\n",
        "        pop_vec = store_pop[store_number]\n",
        "    else:\n",
        "        pop_vec = global_pop\n",
        "    rem = max(0.0, 1.0 - (alpha + beta + gamma))\n",
        "    combined = alpha * score_sim + beta * score_rule + gamma * score_embed + rem * pop_vec\n",
        "    # exclude cart items\n",
        "    for ci in cart_idxs:\n",
        "        combined[ci] = -np.inf\n",
        "    ranked = np.argsort(-combined)\n",
        "    picks = []\n",
        "    for idx in ranked:\n",
        "        if combined[idx] == -np.inf:\n",
        "            continue\n",
        "        picks.append(idx_to_item[idx])\n",
        "        if len(picks) >= top_k:\n",
        "            break\n",
        "    # fillers\n",
        "    if len(picks) < top_k:\n",
        "        fillers = [idx_to_item[i] for i in np.argsort(-global_pop) if idx_to_item[i] not in picks and idx_to_item[i] not in cart_items]\n",
        "        for f in fillers:\n",
        "            picks.append(f)\n",
        "            if len(picks) >= top_k:\n",
        "                break\n",
        "    return picks\n",
        "\n",
        "# Sanity test\n",
        "print('Sanity rec for first tx:', transactions[0][:3], '->', recommend_with_embeddings(transactions[0][:3], 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation helper and grid search (light)\n",
        "import itertools, random, time\n",
        "def recall_at_k_model(sample_tx, k=3, alpha=0.5, beta=0.2, gamma=0.2, samples=1500):\n",
        "    rng = random.Random(42)\n",
        "    cand = [tx for tx in sample_tx if len(tx) >= 2]\n",
        "    n = min(samples, len(cand))\n",
        "    picks = rng.sample(cand, n)\n",
        "    hits = 0\n",
        "    for tx in picks:\n",
        "        hidden = rng.choice(tx)\n",
        "        observed = [t for t in tx if t != hidden]\n",
        "        recs = recommend_with_embeddings(observed, k, alpha, beta, gamma)\n",
        "        if hidden in recs:\n",
        "            hits += 1\n",
        "    return hits / n\n",
        "\n",
        "alphas = [0.4, 0.5, 0.6]\n",
        "betas  = [0.05, 0.1, 0.15]\n",
        "gammas = [0.05, 0.1, 0.15]\n",
        "best = (0, (0,0,0))\n",
        "start = time.time()\n",
        "for a,b,g in itertools.product(alphas, betas, gammas):\n",
        "    if a + b + g >= 1.0: continue\n",
        "    score = recall_at_k_model(transactions, 3, a, b, g, samples=1500)\n",
        "    print(f'a={a}, b={b}, g={g} -> Recall@3: {score:.4f}')\n",
        "    if score > best[0]:\n",
        "        best = (score, (a,b,g))\n",
        "print('\\nBest validation Recall@3:', best, 'time elapsed:', time.time()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final predictions using best found params\n",
        "best_score, best_params = best\n",
        "best_a, best_b, best_g = best_params\n",
        "print('Using best params:', best_a, best_b, best_g, 'val Recall@3:', best_score)\n",
        "\n",
        "out_df = test_df.copy()\n",
        "out_df['RECOMMENDATION 1'] = ''\n",
        "out_df['RECOMMENDATION 2'] = ''\n",
        "out_df['RECOMMENDATION 3'] = ''\n",
        "\n",
        "for idx, row in tqdm(out_df.iterrows(), total=out_df.shape[0], desc='Generating recommendations'):\n",
        "    cart_items = []\n",
        "    for c in item_cols:\n",
        "        v = row.get(c)\n",
        "        if pd.isna(v): continue\n",
        "        s = str(v).strip()\n",
        "        if s == '' or s.lower() == 'missing': continue\n",
        "        cart_items.append(normalize_item(s))\n",
        "    recs = recommend_with_embeddings(cart_items, top_k=3, alpha=best_a, beta=best_b, gamma=best_g)\n",
        "    for j in range(3):\n",
        "        out_df.at[idx, f'RECOMMENDATION {j+1}'] = recs[j] if j < len(recs) else ''\n",
        "\n",
        "out_name = 'Tom and Jerry_Recommendation_Output.xlsx'\n",
        "out_df.to_excel(out_name, index=False)\n",
        "print('Saved', out_name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
